{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from XGboost import XGBRegressor \n",
    "import torch\n",
    "from torch.autograd import grad\n",
    "\n",
    "\n",
    "trainval=pd.read_parquet(\"numerai_training_validation_target_nomi.parquet\")\n",
    "train = trainval[trainval.data_type=='train']\n",
    "\n",
    "target = \"target_nomi\" \n",
    "feature_columns = [c for c in trainval if c.startswith(\"feature\")] \n",
    "\n",
    "# fit an initial model\n",
    "model_init = XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=2000, colsample_bytree=0.1, nthread=6)\n",
    "model_init.fit(train[feature_columns], train[target])\n",
    "\n",
    "# get prediction from initial model as starting point to improve upon\n",
    "base_margin = model_init.predict(train[feature_columns])\n",
    "\n",
    "# get indexes for each era\n",
    "era_idx = [np.where(train.era==uera)[0] for uera in train.era.unique()]\n",
    "\n",
    "\n",
    "# define adjusted sharpe in terms of cost adjusted numerai sharpe\n",
    "def numerai_sharpe(x):\n",
    "    return (x.mean() -0.010415154) / x.std()\n",
    "\n",
    "def skew(x):\n",
    "    mx = x.mean()\n",
    "    m2 = ((x-mx)**2).mean()\n",
    "    m3 = ((x-mx)**3).mean()\n",
    "    return m3/(m2**1.5)    \n",
    "\n",
    "def kurtosis(x):\n",
    "    mx = x.mean()\n",
    "    m4 = ((x-mx)**4).mean()\n",
    "    m2 = ((x-mx)**2).mean()\n",
    "    return (m4/(m2**2))-3\n",
    "\n",
    "def adj_sharpe(x):\n",
    "    return numerai_sharpe(x) * (1 + ((skew(x) / 6) * numerai_sharpe(x)) - ((kurtosis(x) / 24) * (numerai_sharpe(x) ** 2)))\n",
    "\n",
    "# use correlation as the measure of fit\n",
    "def corr(pred, target):\n",
    "    pred_n = pred - pred.mean(dim=0)\n",
    "    pred_n = pred_n / pred_n.norm(dim=0)\n",
    "\n",
    "    target_n = target - target.mean(dim=0)\n",
    "    target_n = target_n / target_n.norm(dim=0)\n",
    "    l = torch.matmul(pred_n.T, target_n)\n",
    "    return l\n",
    "\n",
    "# definte a custom objective for XGBoost\n",
    "def adj_sharpe_obj(ytrue, ypred):\n",
    "    # convert to pytorch tensors\n",
    "    ypred_th = torch.tensor(ypred, requires_grad=True)\n",
    "    ytrue_th = torch.tensor(ytrue)\n",
    "    all_corrs = []\n",
    "\n",
    "    # get correlations in each era\n",
    "    for ee in era_idx:\n",
    "        score = corr(ypred_th[ee], ytrue_th[ee])\n",
    "        all_corrs.append(score)\n",
    "\n",
    "    all_corrs = torch.stack(all_corrs)\n",
    "\n",
    "    # calculate adjusted sharpe using correlations\n",
    "    loss = -adj_sharpe(all_corrs)\n",
    "    print(f'Current loss:{loss}')\n",
    "\n",
    "    # calculate gradient and convert to numpy\n",
    "    loss_grads = grad(loss, ypred_th, create_graph=True)[0]\n",
    "    loss_grads = loss_grads.detach().numpy()\n",
    "\n",
    "    # return gradient and ones instead of Hessian diagonal\n",
    "    return loss_grads, np.ones(loss_grads.shape)\n",
    "\n",
    "\n",
    "model_adj_sharpe = XGBRegressor(max_depth=5, learning_rate=0.01, n_estimators=200, nthread=6, colsample_bytree=0.1, objective=adj_sharpe_obj)\n",
    "model_adj_sharpe.fit(train[feature_columns], train[target], base_margin=base_margin)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
